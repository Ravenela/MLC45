{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4pELSdz7bCRuXL32x42Fb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravenela/MLC45/blob/main/Melanoma_Detection_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PKMRmGUUyfT3"
      },
      "outputs": [],
      "source": [
        "# Aim is to identify melanoma detection from imageset using CNN network\n",
        "#Importing all the important libraries\n",
        "\n",
        "\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DokNh-WS8THK",
        "outputId": "1a2d0b63-1a98-47e7-a1a2-954bf06f0bb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining path of train and test directories\n",
        "\n",
        "data_dir_train = pathlib.Path(\"path_to_train_directory\")\n",
        "data_dir_test = pathlib.Path('path_to_test_directory')  \n",
        "# Checking size of train and test data set\n",
        "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
        "print(image_count_train)\n",
        "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n",
        "print(image_count_test)\n",
        "     \n"
      ],
      "metadata": {
        "id": "vtrOzpuN9UPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27add3b6-d286-4bdd-a73b-c201764056a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading disk into keras module\n",
        "dataset_url = \"https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166\"\n",
        "data_dir = tf.keras.utils.get_file(\n",
        "    origin=dataset_url, untar=True\n",
        ")\n",
        "\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir, image_size=(180, 180), batch_size=64\n",
        ")\n",
        "\n",
        "print(type(dataset))"
      ],
      "metadata": {
        "id": "EDn99dC4Apor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Dataset\n"
      ],
      "metadata": {
        "id": "oAjQENhhIAL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180\n"
      ],
      "metadata": {
        "id": "gvFC7lF_IKju"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = data_dir_train\n",
        "val_ds = data_dir_test\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "y31iGjwIM_M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train test split\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir, image_size=(180, 180), batch_size=64\n",
        ")\n"
      ],
      "metadata": {
        "id": "6x9bJzyPIOHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Model"
      ],
      "metadata": {
        "id": "XnsC3qhZJN7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential\n",
        "model.add (Conv2D (32(3,3), padding =\"same\", input_shape = X_train.shape[1:]))\n",
        "model.add (Activation ('relu'))\n",
        "model.add (Conv2D(32,(3,3)))\n",
        "model.add (Activation ('relu'))\n",
        "model.add (MaxPooling2D(pool_size =(2,2)))\n",
        "model.add (Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add (Activation ('relu'))\n",
        "model.add (Dropout(0.5))\n",
        "model.add (dense(num_classes))\n",
        "model.add(Activation('Softmax'))\n"
      ],
      "metadata": {
        "id": "qJo8i5i-JSlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile the Model"
      ],
      "metadata": {
        "id": "fxsSkxtpLHAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='sgd',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "E6roZmgWLK57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Summary of all layers\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ZfK6P2FQMOLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "_sLR5nA2LrgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "history = model.fit(\n",
        "  X_train,y_train),\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "metadata": {
        "id": "ydDHjUDbL5Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the data\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "     \n"
      ],
      "metadata": {
        "id": "yGtw1LhcMCEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create improved model\n",
        "model = Sequential\n",
        "model.add (Conv2D (32(3,3), padding =\"same\", input_shape = X_train.shape[1:]))\n",
        "model.add (Activation ('relu'))\n",
        "model.add (Conv2D(32,(3,3)))\n",
        "model.add (Activation ('relu'))\n",
        "model.add (MaxPooling2D(pool_size =(2,2)))\n",
        "model.add (Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64,(3,3), padding = 'same'))\n",
        "model.add (Activation('relu'))\n",
        "model.add(Conv2D(64,(3,3)))\n",
        "model.add (Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add (Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add (Activation ('relu'))\n",
        "model.add (Dropout(0.5))\n",
        "model.add (dense(num_classes))\n",
        "model.add(Activation('Softmax'))\n"
      ],
      "metadata": {
        "id": "VzoJ7ImdNT_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "JIdS5ERgPW2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate RMSprop Optimizer\n",
        "opt = keras.optimizers.RMSprop(lr = 0.0001, decay = lr-6)\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Fitting the model\n",
        "model.fit (X_train, y_train), batch_size = batch_size, epochs = epochs, validation_ds = (X_test, y_test), shuffle = True\n"
      ],
      "metadata": {
        "id": "Ai1lFtkIOByp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize dataset\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JOXg6nOSO2-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Augmentor"
      ],
      "metadata": {
        "id": "OwDPapW5PfpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Augmentor\n",
        "     path_to_training_dataset=\"To do\"\n",
        "import Augmentor\n",
        "for i in class_names:\n",
        "    p = Augmentor.Pipeline(path_to_training_dataset + i)\n",
        "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
        "    p.sample(500) ## We are adding 500 samples per class to make sure that none of the classes are sparse.\n",
        "     image_count_train = len(list(data_dir_train.glob('*/output/*.jpg')))\n",
        "print(image_count_train)\n",
        "\n",
        "path_list = [x for x in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]\n",
        "path_list\n",
        "     \n",
        "\n",
        "lesion_list_new = [os.path.basename(os.path.dirname(os.path.dirname(y))) for y in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]\n",
        "lesion_list_new\n",
        "     \n",
        "\n",
        "dataframe_dict_new = dict(zip(path_list_new, lesion_list_new))\n",
        "     \n",
        "\n",
        "df2 = pd.DataFrame(list(dataframe_dict_new.items()),columns = ['Path','Label'])\n",
        "new_df = original_df.append(df2)\n",
        "     \n",
        "\n",
        "new_df['Label'].value_counts()\n",
        "     \n"
      ],
      "metadata": {
        "id": "6xXUWhKDPjog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model using Augmentor\n",
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "\n",
        "# Create Trainig dataset\n",
        "data_dir_train=\"path to directory with training data + data created using augmentor\"\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "  seed=123,\n",
        "  validation_split = 0.2,\n",
        "  subset = ## Todo choose the correct parameter value, so that only training data is refered to,,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "# create a Validation dataset\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "  seed=123,\n",
        "  validation_split = 0.2,\n",
        "  subset = ## Todo choose the correct parameter value, so that only validation data is refered to,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "# Create Model\n",
        "\n",
        "model = Sequential\n",
        "model.add (Conv2D (32(3,3), padding =\"same\", input_shape = X_train.shape[1:]))\n",
        "model.add (Activation ('relu'))\n",
        "model.add (Conv2D(32,(3,3)))\n",
        "model.add (Activation ('relu'))\n",
        "model.add (MaxPooling2D(pool_size =(2,2)))\n",
        "model.add (Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64,(3,3), padding = 'same'))\n",
        "model.add (Activation('relu'))\n",
        "model.add(Conv2D(64,(3,3)))\n",
        "model.add (Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add (Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add (Activation ('relu'))\n",
        "model.add (Dropout(0.5))\n",
        "model.add (dense(num_classes))\n",
        "model.add(Activation('Softmax'))\n",
        "\n",
        "# Compile Model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Fitting the model\n",
        "model.fit (X_train, y_train),\n",
        " batch_size = batch_size, \n",
        " epochs = 30, \n",
        " validation_ds = (X_test, y_test), \n",
        " shuffle = True))"
      ],
      "metadata": {
        "id": "ZdpaJdDqPuqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Model\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "s9leiWAxQqJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model was overfitting earlier. But using Augmentor, Dropout and RMSoptimizer metric, training and validation accuracy improved"
      ],
      "metadata": {
        "id": "SzfaPtyGQxoV"
      }
    }
  ]
}